{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33c5b0c-d0f1-40c0-b794-3b3471ac73d2",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 9: Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086914c2-5de1-414a-8770-23bef9f312d0",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ba588-9919-4524-ba31-11499a9234fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from ‚Äú1‚Äù will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab5740-ae46-4efd-a08e-bf4ae9482701",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594c68-91c3-45d9-baec-6ac38a02c971",
   "metadata": {},
   "source": [
    "## Exercise 1: Communication\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767926d-c17d-4a93-b59a-7242a4c76ff0",
   "metadata": {},
   "source": [
    "### 1.1 Blog post \n",
    "rubric={points:26}\n",
    "\n",
    "Write up your analysis from hw5 or any other assignment or your side project on machine learning in a \"blog post\" or report format. It's fine if you just write it here in this notebook. Alternatively, you can publish your blog post publicly and include a link here. (See exercise 1.3.) The target audience for your blog post is someone like yourself right before you took this course. They don't necessarily have ML knowledge, but they have a solid foundation in technical matters. The post should focus on explaining **your results and what you did** in a way that's understandable to such a person, **not** a lesson trying to teach someone about machine learning. Again: focus on the results and why they are interesting; avoid pedagogical content.\n",
    "\n",
    "Your post must include the following elements (not necessarily in this order):\n",
    "\n",
    "- Description of the problem/decision.\n",
    "- Description of the dataset (the raw data and/or some EDA).\n",
    "- Description of the model.\n",
    "- Description your results, both quantitatively and qualitatively. Make sure to refer to the original problem/decision.\n",
    "- A section on caveats, describing at least 3 reasons why your results might be incorrect, misleading, overconfident, or otherwise problematic. Make reference to your specific dataset, model, approach, etc. To check that your reasons are specific enough, make sure they would not make sense, if left unchanged, to most students' submissions; for example, do not just say \"overfitting\" without explaining why you might be worried about overfitting in your specific case.\n",
    "- At least 3 visualizations. These visualizations must be embedded/interwoven into the text, not pasted at the end. The text must refer directly to each visualization. For example \"as shown below\" or \"the figure demonstrates\" or \"take a look at Figure 1\", etc. It is **not** sufficient to put a visualization in without referring to it directly.\n",
    "\n",
    "A reasonable length for your entire post would be **800 words**. The maximum allowed is **1000 words**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169eefb-18a8-4e13-b7ca-1b3539a79215",
   "metadata": {},
   "source": [
    "#### Example blog posts\n",
    "\n",
    "Here are some examples of applied ML blog posts that you may find useful as inspiration. The target audiences of these posts aren't necessarily the same as yours, and these posts are longer than yours, but they are well-structured and engaging. You are **not required to read these** posts as part of this assignment - they are here only as examples if you'd find that useful.\n",
    "\n",
    "From the UBC Master of Data Science blog, written by a past student:\n",
    "\n",
    "- https://ubc-mds.github.io/2019-07-26-predicting-customer-probabilities/\n",
    "\n",
    "This next one uses R instead of Python, but that might be good in a way, as you can see what it's like for a reader that doesn't understand the code itself (the target audience for your post here):\n",
    "\n",
    "- https://rpubs.com/RosieB/taylorswiftlyricanalysis\n",
    "\n",
    "Finally, here are a couple interviews with winners from Kaggle competitions. The format isn't quite the same as a blog post, but you might find them interesting/relevant:\n",
    "\n",
    "- https://medium.com/kaggle-blog/instacart-market-basket-analysis-feda2700cded\n",
    "- https://medium.com/kaggle-blog/winner-interview-with-shivam-bansal-data-science-for-good-challenge-city-of-los-angeles-3294c0ed1fb2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdd094-eeb6-4f00-a3bc-5a6105eedb12",
   "metadata": {},
   "source": [
    "#### A note on plagiarism\n",
    "\n",
    "You may **NOT** include text or visualizations that were not written/created by you. If you are in any doubt as to what constitutes plagiarism, please just ask. For more information see the [UBC Academic Misconduct policies](http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,959). Please don't copy this from somewhere or ask Generative AI to write it for you üôè. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052395d-a695-4063-97b6-46c4e13016d8",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258d031-b949-4a7a-b259-9f3685c27ed4",
   "metadata": {},
   "source": [
    "Turning Meaningless Text into Meaningful Text for Machine Learning Models: Clustering and Unsupervised Learning\n",
    "By Nathan Sihombing ‚Äì June 2025\n",
    "(Based on Homework 6 ‚Äì CPSC 330: Unsupervised Learning and Clustering)\n",
    "\n",
    "The Setup and Background\n",
    "This blog post reflects on what I did and learned during Homework 6 from CPSC 330. The goal of the assignment was to practice unsupervised learning on text data. A lot of the code and structure were scaffolded for us (as a disclaimer for my use of \"we\"). But within that structure, there was room to reflect on what worked, what didn‚Äôt, and how different representations and models affected the outcome.\n",
    "We focused on two datasets: Wikipedia sentences and recipe names. In both cases, the task was the same: try to find meaningful clusters in unlabeled text data, using a combination of sentence embeddings and different clustering algorithms. Very different from supervised models, where the data has been previously labelled. But in situations with text data, most of the time, they‚Äôll require unsupervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153c0c1-7285-4ee1-8b6e-92552e06461d",
   "metadata": {},
   "source": [
    "The Datasets\n",
    "The first dataset was a small collection of Wikipedia sentences. They were short and mixed, covering topics like history, science, politics, music, and food. The second dataset was a longer list of recipe titles, which were often very short and lacked meaningful context.\n",
    "To work with the text, we used sentence embeddings. So basically, each sentence or recipe title was already turned into a 384-dimensional vector that kind of captured what it was about. Since the embeddings were given to us, we didn‚Äôt have to worry about feature engineering; we could just jump straight into clustering. Implementing and sourcing embeddings from other places is always helpful to save time when doing larger-scale models.\n",
    "\n",
    "Manual Grouping\n",
    "We started by manually grouping a short list of Wikipedia sentences into clusters just as a warm-up to get a feel for how things might be grouped. I came up with four groups: AI/ML, quantum computing, environmental science, and food. It‚Äôs very good as a programmer to manually manipulate and look through your data. If you understand your data, then you will know what to look for. \n",
    "KMeans with Bag-of-Words\n",
    "We first tried KMeans using a basic bag-of-words representation. The results weren‚Äôt great as the clusters felt random and didn‚Äôt make a lot of sense, which in hindsight makes sense. BoW strips away context and order, so it didn‚Äôt capture much meaning. That ended up with nonsense clusters. \n",
    "Sentence Embeddings with KMeans\n",
    "We then reran K-Means, but this time using sentence embeddings. That changed everything. Now, some clusters were clearly about military history, others were about music or science. It was the first time I saw how much difference a better representation can really make.\n",
    "DBSCAN\n",
    "Next, we tried DBSCAN. It was better at carving out smaller, tighter clusters and marking others as noise. It didn‚Äôt need us to pick k, which was nice, but tuning eps took some work. What was great was the inclusion of an outlier class; this would be helpful in this context to prevent outliers from skewing the clusters. \n",
    "Hierarchical Clustering\n",
    "We applied hierarchical clustering using cosine distance. The dendrogram helped visualize how sentences grouped at different levels. Some links felt random, but some made sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fe89e-0d45-452a-9e80-b5610c3843a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"img/m2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39428938-a781-4eef-be5e-582ea6340483",
   "metadata": {},
   "source": [
    "This is a very helpful tool to understand where to cut off at the distance, where each data point clusters with other data points. Especially when you do Hierarchical clustering, the dendogram makes the clustering super interpretable. Honestly making it my favourite type of model\n",
    "To wrap up the Wikipedia sentence clustering section, I used UMAP to project the high-dimensional embeddings into 2D. Then I visualized the clusters from different methods to see how things looked spatially.\n",
    "\n",
    "Here‚Äôs what I saw:\n",
    "Bag-of-Words + KMeans looked pretty scattered. Most of the points ended up in a single cluster, and the rest felt randomly assigned. It didn‚Äôt really separate meaningful topics, which confirmed that BoW just isn‚Äôt good enough for this kind of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f878b5-ffb4-4e72-a665-0448176a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"img/m1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ecc334-198c-4872-91fc-0d9b91a3a462",
   "metadata": {},
   "source": [
    "\n",
    "Embeddings + KMeans looked a lot better. The clusters were way more balanced, and I could already see groupings that matched themes like music, politics, or history.\n",
    "\n",
    "\n",
    "Embeddings + DBSCAN picked out a few really tight clusters and labeled a bunch of points as noise (-1). That actually made sense, as some sentences were too vague or didn‚Äôt clearly belong to any topic.\n",
    "\n",
    "\n",
    "Embeddings + Hierarchical gave a nice spread. It wasn‚Äôt as sharp as DBSCAN, but still formed some logical groupings. Some clusters were mixed, but overall, the separation was decent.\n",
    "\n",
    "\n",
    "What stood out the most was how switching from bag-of-words to sentence embeddings completely changed the structure. Just by changing the representation, the clusters went from random to meaningful.\n",
    "This visual check really helped solidify that the embedding space captured actual topic similarity and that clustering methods like DBSCAN and hierarchical could work well if the representation was good to begin with.\n",
    "We looked at the shortest and longest recipe names, then generated a new set of sentence embeddings for all titles. These recipe titles were a lot shorter than the Wikipedia ones, so clustering them felt more challenging from the start. That helped me realize how vague and minimal a lot of these titles were, which probably made it harder for models to form clear clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbeebd4-7cd5-4f3f-9b29-489c6e6485e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"img/m4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6359979-4807-4beb-ad94-582e19d7614a",
   "metadata": {},
   "source": [
    "For KMeans, I tested cluster sizes from k = 2 to k = 10. I calculated both inertia (for the elbow plot) and silhouette scores to see what value of k made the most sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d64e5-d324-4bfd-983c-f88103f7972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"img/m3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79e110-808f-4627-b448-9d1e943941d1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The elbow plot didn't show a super clear \"elbow\", but the curve started to flatten around k = 10, so that seemed like a reasonable choice. The silhouette score peaked at k = 2 (~0.046) and dipped around k = 4, but it slowly increased again, ending around ~0.026 at k = 10.\n",
    "Even though that wasn‚Äôt the highest score, I went with k = 10 because the clusters were more interpretable and better aligned with real themes, like desserts, soups, drinks, and meat dishes. Some examples, like chicken recipes, salads, and soups, are all separated into their own groups, which helps reinforce that the choice made sense.\n",
    "In hindsight, I think higher values of k might have worked, too, since some recipe categories can be really specific. But at the same time, I didn‚Äôt want to risk over-splitting the data when k = 10 gave me solid results.\n",
    "\n",
    "DBSCAN\n",
    "I experimented with a few different values of eps and min_samples to see what would work best for DBSCAN using cosine distance. After testing, I settled on eps = 0.3 and min_samples = 5, which gave me 18 clusters along with a few noise points.\n",
    "The silhouette score came out slightly negative at ‚Äì0.1277, which usually means the clustering isn't very tight or well-separated. But when I looked at the clusters manually, they still made sense. Which really showed me how scores don‚Äôt always reflect the full picture, especially with short text.\n",
    "Some specific groupings that DBSCAN captured well included:\n",
    "Cabbage dishes\n",
    "\n",
    "\n",
    "Pilafs\n",
    "\n",
    "\n",
    "Cocktails\n",
    "\n",
    "\n",
    "Pork medallions\n",
    "\n",
    "\n",
    "Meatloaf\n",
    "\n",
    "\n",
    "Because DBSCAN is great at finding dense, small clusters, it actually fit this dataset well, and a lot of recipes fall naturally into specific categories that don‚Äôt need a large cluster to be meaningful. So while the silhouette score didn‚Äôt look good on paper, the clusters themselves were sharp and well-defined, especially compared to KMeans which tended to group broader themes.\n",
    "Hierarchical Clustering\n",
    "For hierarchical clustering, I used the sentence embeddings and applied linkage with method=\"complete\" and metric=\"cosine\". I first tried pruning the dendrogram using a distance-based cutoff (t=0.97, criterion=\"distance\"), but I found it hard to choose a good threshold this way. Small distance changes resulted in big jumps in the number of clusters, and it wasn‚Äôt very obvious where to cut based on the dendrogram‚Äôs shape.\n",
    "Because of that, I switched to using maxclust=17 to directly specify the number of clusters, which was easier to tune and gave me a more stable and interpretable result. The clusters I got mostly lined up with the themes I saw in the KMeans and DBSCAN results.\n",
    "Some of the dominant themes in the clusters included:\n",
    "Drinks\n",
    "\n",
    "\n",
    "Baked goods\n",
    "\n",
    "\n",
    "Meat dishes\n",
    "\n",
    "\n",
    "Seafood\n",
    "\n",
    "\n",
    "Salads\n",
    "\n",
    "\n",
    "Weight Watchers recipes\n",
    "\n",
    "\n",
    "However, since recipe names are often short and vague, not all clusters were clean. For example, I had a few clusters that mixed together drinks and desserts, or grouped appetizers with unrelated dishes. This makes sense when working with short text; sometimes, the name alone doesn‚Äôt give the model enough to separate items perfectly.\n",
    "Overall, hierarchical clustering gave me reasonably meaningful groupings and helped reinforce what I learned from the other clustering methods, especially in terms of which themes consistently showed up across approaches.\n",
    "Finally, we were asked to label the clusters ourselves. This part was kind of subjective, and some clusters were easy to name (e.g., desserts), others were a mix. But it was a good reminder that interpretation still matters even in \"automatic\" models.\n",
    "\n",
    "\n",
    "I learned to trust visuals and manual inspection just as much as numbers, especially when working with messy or ambiguous data like recipe names. But I also personally believe it‚Äôs much more applicable to say this for unsupervised learning. \n",
    "\n",
    "Limitations\n",
    "No labels means no ‚Äúaccuracy‚Äù: Evaluation was entirely subjective\n",
    "\n",
    "\n",
    "Short text is hard to interpret: Even embeddings have limits\n",
    "\n",
    "\n",
    "UMAP is good, but misleading: Great for plots, not for distances so you have to know what you are looking at. \n",
    "\n",
    "\n",
    "Model sensitivity: Especially DBSCAN.  The smallest changes = big shifts\n",
    "This assignment gave me more than practice, and it changed how I think about data analysis. It showed that unsupervised learning can reveal real structure, even in text. Most importantly, it reminded me that features matter more than models. You can have the best algorithm, but if the representation is bad, the result will be too. So, as an ML scientist, following the process is just as important as picking the correct model. \n",
    "I didn‚Äôt go into this expecting to enjoy clustering. But what I found was satisfying. Being able to find meaningful patterns in unstructured data. Watching the data sort itself out was both functional and fun to do. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59667c9-db6a-4c12-a556-5b9815ef3564",
   "metadata": {},
   "source": [
    "### 1.2 Effective communication technique\n",
    "rubric={points:4}\n",
    "\n",
    "Describe one effective communication technique that you used in your post, or an aspect of the post that you are particularly satisfied with. (Max 3 sentences.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b82c8-4713-4314-9b76-a3a72a6b93eb",
   "metadata": {},
   "source": [
    "I really liked the plots I used, and I think it would help visual the ideas. Another thing I believe I did well was use communicative language that a newbie Machine Learning Student can understand this blog post. Being able to explain your thought process effeciently can be hard sometimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea9c37-34c9-4b3e-a2df-e00cedd3e8ae",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56965b8c-9f4d-4a68-be4c-2a745dcf9989",
   "metadata": {},
   "source": [
    "### (optional, not for marks) 1.3\n",
    "\n",
    "Publish your blog post from 1.1 publicly using a tool like [Quarto](https://quarto.org/), or somewhere like medium.com, and paste a link here. Be sure to pick a tool in which code and code output look reasonable. This link could be a useful line on your resume!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a55e63-38cb-4e36-867f-0261d1c583de",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cefc8e-cf76-4c27-9aa6-c560cbc0fc2b",
   "metadata": {},
   "source": [
    "## Exercise 2: Your takeaway from the course \n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Reflect on your journey through this course. Please identify and elaborate on at least three key concepts or experiences where you had an \"aha\" moment. How would you use the concepts learned in this course in your personal projects or how would you approach your past projects differently based on the insights gained in this course? We encourage you to dig deep and share your genuine reflections.\n",
    "\n",
    "> Please write thoughtful answers. We are looking forward to reading them üôÇ. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eba75a6a-2775-4ffb-9123-d53192df5df1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2fb9e2f-a2d2-4f56-9fb4-c91492e4801b",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d270c-305d-463b-9db5-9802d70d79bf",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e160c-d947-4123-8e67-fa3c89c9aa8f",
   "metadata": {},
   "source": [
    "### Congratulations üëèüëè\n",
    "\n",
    "That's all for the assignments! Congratulations on finishing all homework assignments! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3bee4-0171-4465-838f-e5ac8a943e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"img/eva-congrats.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
